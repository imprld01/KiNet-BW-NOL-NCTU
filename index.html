<!doctype html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8;">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<meta name="author" content="Bo-Wun Shih">
		<meta name="description" content="">
		<meta name="keywords" content="KiNet">
		<meta name="msvalidate.01" content="AA88D87AFAF648FEE45BA3A5A98B147B" />
		<meta name="google-site-verification" content="qsJQCL11a0qVDx46trPNIEed4YnldTyB7pPn0OxHtec" />
		
		<title>KiNet</title>
		
		<link rel="shortcut icon" href="assets/img/o.ico">
		<link rel="stylesheet" href="assets/css/bwlp.css">
		
		<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
		<script>
			(adsbygoogle = window.adsbygoogle || []).push({
				google_ad_client: "ca-pub-6015636098022992",
				enable_page_level_ads: true
			});
		</script>
	</head>
	
	<body>
		<table class="body-wrap">
			<tr><td class="container">
				<table>
					<tr><td align="center" class="masthead">
							<h1>應用機器學習機器技術於智慧健身教練系統</h1>
							<h2>Machine-Train-Machine Application on Fitness E-Coaching System</h2>
							<h4><a href="https://imprld01.github.io/about" style="color: #3474db" target="_blank">施博文</a>, <a href="https://www.cs.nctu.edu.tw/cswebsite/members/detail/yi" style="color: #3474db" target="_blank">Tsì-Uí I̍k</a></h4>
					</td></tr>
					<tr><td class="content">
						<h2>Introduction</h2>
						<p>
							Kinect先天具有使用環境、硬體介面、生產成本及攜帶性等硬體侷限問題。
							研究運用Machine-Train-Machine概念，取得Kinect的原始資料進行訓練，試圖取代Kinect的人物姿勢與深度資訊辨識功能。
							研究使用Multi-task Learning技術建置深度學習模型，由彩色影像同時估算人物姿勢與深度資訊。
							<img src="assets/img/intro.jpg" alt="intro" style="margin-top: 20px">
						</p>
						<p>
							藉由多任務學習架構，多項不同任務的前段特徵抽取架構設計適合共用。
							參考近年有效模型架構設計，可以建立一個可以同時用來偵測人物姿勢與深度影像的系統。
							<img src="assets/img/concept.JPG" alt="intro" style="margin-top: 20px">
						</p>
						
						<h2>Resource</h2>
						<p>
							針對人物姿勢估測以及深度圖預測兩項任務，網路可以取得不錯的開放資料集，包含MPII與NYU Depth等資料集。
							MPII資料集提供豐富的人類活動影像 (RGB Image)與對應的人物關節註釋 (Annotation)。
							NYU Depth資料集收集有彩色影像 (RGB Image)與深度資訊 (Depth Information)。
						</p>
						<p>
							研究除了使用開源資料之外，也自行收集Kinect v2的原始感測資料，包含彩色影像、深度影像與人體關節註釋等資料。
						</p>
						<table class="thetable" style="margin-bottom: 30px;">
							<caption>資料集資源 (Dataset Resource)</caption>
							<tr>
								<td width="5%">資料集</td>
								<td>內容</td>
								<td width="8%">關節數</td>
								<td>景深距離</td>
								<td width="10%">樣本數</td>
								<td>原始來源 (Open Data)</td>
								<td>檔案格式</td>
								<td width="5%">備份源</td>
							</tr>
							<tr>
								<td>MPII Dataset</td>
								<td style="text-align: left;">
									1. Color Image<br/>
									2. Joint Annotation<br/>
								</td>
								<td>16 joints</td>
								<td>-</td>
								<td>28883</td>
								<td style="text-align: left;">
									<a href="http://human-pose.mpi-inf.mpg.de/#download" target="_blank">RGB Image & Annotation</a><br/>
									<a href="http://www.cims.nyu.edu/~tompson/data/mpii_valid_pred.zip" target="_blank">Validation</a>
								</td>
								<td>MAT</td>
								<td>
									<a href="https://drive.google.com/open?id=1TPHkuDoO_zqM8jlpynhWVUt_ZGVfBNCn" target="_blank">NOL Backup</a>
								</td>
							</tr>
							<tr>
								<td>NYU-Depth Dataset</td>
								<td style="text-align: left;">
									1. Color Image<br/>
									2. Depth Map
								</td>
								<td>-</td>
								<td>0.8 ~ 4.0 m</td>
								<td>3733<br/>(v1: 2284, v2: 1449)</td>
								<td style="text-align: left;">
									<a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v1.html" target="_blank">NYU V1</a><br/>
									<a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" target="_blank">NYU V2</a><br/>
								</td>
								<td>MAT</td>
								<td>
									<a href="https://drive.google.com/open?id=1J03riMqOzlO9Uc0SIkTrlufDu8CrLRhf" target="_blank">NOL Backup</a>
								</td>
							</tr>
							<tr>
								<td>NCTU Fitness Dataset</td>
								<td style="text-align: left;">
									1. Color Image<br/>
									2. Joint Annotation<br/>
									3. Depth Map<br/>
									4. Binocular Color Image<br/>
									5. IMU Data<br/>
									6. Mobile Video
								</td>
								<td>25 joints</td>
								<td>0.5 ~ 4.5 m</td>
								<td>47131</td>
								<td style="text-align: left;">
									<a href="https://drive.google.com/drive/folders/1v2en12WdcymzYGOiVmRUtHNGzQYV6tJj" target="_blank">Fitness</a><br/>
								</td>
								<td>CSV</td>
								<td>
									<a href="https://imprld01.github.io/Construction-Page" target="_blank">NOL Backup</a>
								</td>
							</tr>
							<tr>
								<td>NOL Kinect Dataset</td>
								<td style="text-align: left;">
									1. Color Image<br/>
									2. Joint Annotation<br/>
									3. Depth Map
								</td>
								<td>25 joints</td>
								<td>0.5 ~ 4.5 m</td>
								<td>???</td>
								<td style="text-align: left;">
									-
								</td>
								<td>CSV</td>
								<td>
									<a href="https://imprld01.github.io/Construction-Page" target="_blank">NOL Backup</a>
								</td>
							</tr>
						</table>
						<p>
							Code: <a href="https://gitlab.com/imprld01/KiNet" target="_blank">here</a>.<br>
							Paper: <a href="https://imprld01.github.io/Construction-Page" target="_blank">here</a>.<br>
							Docker: <span style="color: #71bc37">imprld01/kinet:v1</span>.
						</p>
						
						<h2>Contact</h2>
						<p>Please send any questions or comments to Bo-Wun at <a href="&#109;ail&#116;o&#58;sbw%&#51;2%&#51;3&#49;9&#64;&#103;&#37;6D&#37;61il&#46;%63%6&#70;&#37;&#54;D">&#115;bw231&#57;(at&#41;gma&#105;l(dot)com</a>.</p>
					</td></tr>
				</table>
			</td></tr>
			<tr><td class="container">
				<table><tr><td class="content footer" style="padding: 0px 120px;" align="center">
					<p>Maintained by <a href="https://github.com/imprld01" target="_blank">Bo-Wun Shih</a> @ 2019/01/16</p>
					<p>Designed by <a href="https://github.com/imprld01" target="_blank">Bo-Wun Shih</a> @ 2019/01/16</p>
					<p>Powered by <a href="https://github.com/derekpunsalan/responsive-email" target="_blank">derekpunsalan/responsive-email</a></p>
					<p>Copyright &copy; 2019 <a href="https://imprld01.github.io">Bo-Wun</a>, LICENSE under <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> and <a href="https://github.com/imprld01/imprld01.github.io/blob/master/LICENSE" target="_blank">MIT</a></p>
				</td></tr></table>
			</td></tr>
		</table>
	</body>
</html>
